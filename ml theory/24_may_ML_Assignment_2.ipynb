{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is regression analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression analysis is a statistical technique used to model and analyze the relationships between a dependent variable and one or more independent variables. It helps in predicting the value of the dependent variable based on the values of the independent variables and is widely used in predictive modeling and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Explain the difference between linear and nonlinear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear Regression models the relationship between dependent and independent variables as a straight line. It assumes that changes in the independent variable(s) produce proportional changes in the dependent variable.\n",
    "\n",
    "* Nonlinear Regression models the relationship as a curve or some other complex mathematical function. It is used when the relationship between variables is not linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between simple linear regression and multiple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple Linear Regression involves one independent variable and one dependent variable, with a linear relationship between them.\n",
    "\n",
    "* Multiple Linear Regression involves two or more independent variables and one dependent variable, modeling the linear relationship between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How is the performance of a regression model typically evaluated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The performance is evaluated using metrics such as:\n",
    "\n",
    "   * Mean Absolute Error (MAE)\n",
    "   * Mean Squared Error (MSE)\n",
    "   * Root Mean Squared Error (RMSE)\n",
    "   * R-squared (R²) These metrics measure the average difference between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What is overfitting in the context of regression models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overfitting occurs when a model learns the noise in the training data instead of the underlying pattern. This leads to a model that performs well on training data but poorly on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. What is logistic regression used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression is used for binary classification problems, where the output variable is categorical (e.g., yes/no, true/false). It estimates the probability that a given input point belongs to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. How does logistic regression differ from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear Regression predicts continuous outcomes and assumes a linear relationship between the dependent and independent variables.\n",
    "* Logistic Regression predicts binary outcomes and uses the logistic function (sigmoid function) to map predicted values to probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Explain the concept of odds ratio in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Odds Ratio represents the ratio of the odds of an event occurring to the odds of it not occurring. In logistic regression, it quantifies how a one-unit change in an independent variable affects the odds of the dependent variable being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. What is the sigmoid function in logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Sigmoid Function is used in logistic regression to map predicted values to probabilities, which range from 0 to 1. It is defined as:\n",
    "\n",
    "   'σ(x) = 1/(1+e^−x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. How is the performance of a logistic regression model evaluated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Performance metrics include:\n",
    "\n",
    "  *  Accuracy\n",
    "  *  Precision\n",
    "  *  Recall\n",
    "  *  F1 Score\n",
    "  *  ROC-AUC (Receiver Operating Characteristic - Area Under Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11. What is a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It models decisions and their possible consequences using a tree-like structure of nodes (tests on features) and branches (outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12. How does a decision tree make predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Decision Tree makes predictions by traversing the tree from the root node to a leaf node, based on the values of the input features and the conditions defined at each internal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13. What is entropy in the context of decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entropy is a measure of impurity or randomness in the dataset. In decision trees, entropy is used to determine the best feature to split the data, aiming to reduce the impurity in each subsequent split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14. What is pruning in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pruning is a technique used to reduce the size of a decision tree by removing sections that provide little power in classifying instances. It helps prevent overfitting by simplifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15. How do decision trees handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision trees can handle missing values by:\n",
    "    \n",
    "    * Using surrogate splits to find the best alternative feature when a value is missing.\n",
    "    * Imputing missing values with the most frequent value in the dataset or another statistical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16. What is a support vector machine (SVM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that separates different classes in the feature space with the maximum margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17. Explain the concept of margin in SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Margin in SVM refers to the distance between the separating hyperplane and the closest data points from each class. The goal is to maximize this margin to improve the classifier's robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18. What are support vectors in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Support Vectors are the data points closest to the hyperplane and most critical in defining the decision boundary. These points directly influence the position and orientation of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19. How does SVM handle non-linearly separable data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM handles non-linearly separable data by using the Kernel Trick. Kernels transform the data into a higher-dimensional space where a linear separation is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20. What are the advantages of SVM over other classification algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages of SVM include:\n",
    "    * Effective in high-dimensional spaces.\n",
    "    * Robust to overfitting, especially in high-dimensional space.\n",
    "    * Can handle both linear and non-linear classification using different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q21. What is the Naïve Bayes algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Naïve Bayes algorithm is a probabilistic classifier based on Bayes' Theorem. It assumes that the features are conditionally independent given the class label, which simplifies the computation of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q22. Why is it called \"Naïve\" Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is called Naïve because it makes the simplifying assumption that all features are conditionally independent of each other given the class label, which is often not the case in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q23. How does Naïve Bayes handle continuous and categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes handles:\n",
    "    * Continuous Features by assuming they follow a normal distribution and applying Gaussian Naïve Bayes.\n",
    "    * Categorical Features using multinomial or Bernoulli distributions, depending on the nature of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q24. Explain the concept of prior and posterior probabilities in Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prior Probability is the probability of a class before observing any data.\n",
    "* Posterior Probability is the updated probability of a class after observing the data, computed using Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q25. What is Laplace smoothing and why is it used in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Laplace Smoothing is a technique used to handle zero-frequency problems in Naïve Bayes by adding a small constant to all counts, ensuring no probability is ever zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q26. Can Naïve Bayes be used for regression tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes is primarily a classification algorithm. It is not typically used for regression tasks because it is based on the concept of probability distributions, which is more suited to discrete class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q27. How do you handle missing values in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missing values in Naïve Bayes can be handled by:\n",
    "    * Ignoring the missing values for that feature.\n",
    "    * Imputing missing values using statistical methods like mean, median, or mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q28. What are some common applications of Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common applications include:\n",
    "    * Text classification (e.g., spam detection).\n",
    "    * Sentiment analysis.\n",
    "    * Document categorization.\n",
    "    * Medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q29. Explain the concept of feature independence assumption in Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Feature Independence Assumption assumes that all features contribute independently to the probability of a class label, given the class label. This assumption simplifies computation but may not always hold in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q30. How does Naïve Bayes handle categorical features with a large number of categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes can handle categorical features with many categories by computing the probability of each category separately, but it may become less effective due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q31. What is the curse of dimensionality, and how does it affect machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Curse of Dimensionality refers to the phenomenon where the volume of the feature space increases exponentially with the number of features, making data sparse and difficult to generalize from, affecting the performance of many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q32. Explain the bias-variance tradeoff and its implications for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Bias-Variance Tradeoff is the balance between:\n",
    "    * Bias: Error due to overly simplistic assumptions in the model, leading to underfitting.\n",
    "    * Variance: Error due to the model's sensitivity to small fluctuations in the training data, leading to overfitting. A good model should minimize both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q33. What is cross-validation, and why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross-Validation is a technique used to evaluate the performance of a model by dividing the data into multiple subsets. The model is trained on some subsets and tested on others, ensuring that the evaluation is robust and not dependent on a particular subset of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q34. Explain the difference between parametric and non-parametric machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parametric Algorithms assume a fixed form for the model (e.g., linear regression) and learn a finite number of parameters from the data.\n",
    "\n",
    "* Non-Parametric Algorithms do not assume a specific form for the model (e.g., decision trees, KNN) and can grow in complexity with the amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q35. What is feature scaling, and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Scaling is the process of normalizing or standardizing the range of features in the data. It is important because many machine learning algorithms (like SVM, KNN) are sensitive to the scale of the data, and unscaled features can lead to biased models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q36. What is regularization, and why is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. It discourages overly complex models by penalizing large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q37. Explain the concept of ensemble learning and give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensemble Learning is a technique where multiple models (often of the same type) are combined to improve overall performance. An example is the Random Forest, which combines multiple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q38. What is the difference between bagging and boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bagging (Bootstrap Aggregating): Reduces variance by training multiple models on random subsets of data and aggregating their results (e.g., Random Forest).\n",
    "\n",
    "* Boosting: Reduces both bias and variance by sequentially training models, with each new model focusing on the errors of the previous one (e.g., AdaBoost, Gradient Boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q39. What is the difference between a generative model and a discriminative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generative Models (e.g., Naïve Bayes) model the joint probability distribution of input features and output labels.\n",
    "\n",
    "* Discriminative Models (e.g., logistic regression, SVM) model the conditional probability of the output given the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q40. Explain the concept of batch gradient descent and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Gradient Descent computes the gradient using the entire training dataset and updates the model parameters.\n",
    "* Stochastic Gradient Descent (SGD) computes the gradient using a single training example (or a small batch) and updates the model parameters, leading to faster convergence but more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q41. What is the K-nearest neighbors (KNN) algorithm, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The K-Nearest Neighbors (KNN) algorithm is a non-parametric method used for classification and regression. It works by finding the k closest training examples in the feature space to a given input and making predictions based on their majority class or average value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q42. What are the disadvantages of the K-nearest neighbors algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Disadvantages of KNN include:\n",
    "    * High computational cost during prediction, as it requires distance calculation with all training points.\n",
    "    * Sensitivity to irrelevant or redundant features.\n",
    "    * Poor performance on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q43. Explain the concept of one-hot encoding and its use in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-Hot Encoding is a technique used to convert categorical variables into a binary format that machine learning algorithms can understand. Each category is represented by a binary vector with a single 1 and the rest as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q44. What is feature selection, and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Selection is the process of selecting a subset of relevant features for building the model. It is important because it reduces model complexity, improves performance, and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q45. Explain the concept of cross-entropy loss and its use in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross-Entropy Loss measures the difference between two probability distributions, typically the predicted and true distributions. It is used in classification tasks to optimize models for predicting class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q46. What is the difference between batch learning and online learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Learning: The model is trained on the entire dataset at once, which requires all data to be available before training.\n",
    "\n",
    "* Online Learning: The model is trained incrementally as new data arrives, suitable for dynamic or real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q47. Explain the concept of grid search and its use in hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Grid Search is a technique for hyperparameter tuning that exhaustively searches through a manually specified subset of the hyperparameter space of a model to find the best combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q48. What are the advantages and disadvantages of decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages: Easy to interpret, handle both numerical and categorical data, and require little data preprocessing.\n",
    "* Disadvantages: Prone to overfitting, can be unstable with small data changes, and may not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q49. What is the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1 Regularization (Lasso) adds a penalty equal to the absolute value of the coefficients, promoting sparsity in the model.\n",
    "* L2 Regularization (Ridge) adds a penalty equal to the square of the coefficients, discouraging large weights but allowing smaller, non-zero weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q50. What are some common preprocessing techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common preprocessing techniques include:\n",
    "    * Feature Scaling (Normalization and Standardization).\n",
    "    * Imputation of missing values.\n",
    "    * Encoding categorical features (One-Hot, Label Encoding).\n",
    "    * Feature Engineering and Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q51. What is the difference between a parametric and non-parametric algorithm? Give examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parametric Algorithms: Assume a fixed number of parameters (e.g., Linear Regression, Logistic Regression).\n",
    "* Non-Parametric Algorithms: Do not assume a fixed form or parameters (e.g., KNN, Decision Trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q52. Explain the bias-variance tradeoff and how it relates to model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bias-Variance Tradeoff: A more complex model can capture more patterns (low bias) but may also capture noise (high variance). A simpler model may have high bias but low variance. The goal is to find the right complexity to minimize both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q53. What are the advantages and disadvantages of using ensemble methods like random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages: Improve prediction accuracy, reduce overfitting, and provide robust results.\n",
    "* Disadvantages: Can be computationally expensive and less interpretable than single models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q54. Explain the difference between bagging and boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bagging: Aggregates predictions of multiple base models to reduce variance (e.g., Random Forest).\n",
    "\n",
    "* Boosting: Sequentially builds models that focus on the errors of previous models to reduce both bias and variance (e.g., Gradient Boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q55. What is the purpose of hyperparameter tuning in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameter Tuning aims to find the best combination of hyperparameters that maximize the model's performance on a validation set, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q56 Hyperparameter Tuning aims to find the best combination of hyperparameters that maximize the model's performance on a validation set, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regularization penalizes complex models to prevent overfitting.\n",
    "* Regularization penalizes complex models to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q57. How does the Lasso (L1) regularization differ from Ridge (L2) regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lasso (L1) Regularization adds a penalty equal to the absolute value of the coefficients, leading to sparsity (some coefficients are zero).\n",
    "\n",
    "* Ridge (L2) Regularization adds a penalty equal to the square of the coefficients, shrinking all coefficients but rarely leading to zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q58. Explain the concept of cross-validation and why it is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross-Validation is used to evaluate a model's performance by dividing the data into multiple subsets. The model is trained on some subsets and tested on others, providing a robust measure of its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q59. What are some common evaluation metrics used for regression tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common metrics include:\n",
    "    * Mean Absolute Error (MAE)\n",
    "    * Root Mean Squared Error (RMSE)\n",
    "    * R-squared (R²)\n",
    "    * Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 60. How does the K-nearest neighbors (KNN) algorithm make predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN predicts the output for a data point by finding the k closest points in the training data and taking the majority class (for classification) or the average value (for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q61. What is the curse of dimensionality, and how does it affect machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Curse of Dimensionality refers to the exponential increase in data sparsity as the number of dimensions (features) grows. It makes distance-based learning methods like KNN less effective and increases the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q62. What is feature scaling, and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Scaling standardizes the range of independent variables or features. It is important because it ensures that all features contribute equally to the model, especially for distance-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q63. How does the Naïve Bayes algorithm handle categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes handles categorical features by estimating the probability of each category given the class label, using the frequency of occurrences in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q64. Explain the concept of prior and posterior probabilities in Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prior Probability: The initial probability of a class before observing any data.\n",
    "* Posterior Probability: The updated probability of a class after observing data, calculated using Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q65. What is Laplace smoothing, and why is it used in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Laplace Smoothing prevents zero probabilities in Naïve Bayes by adding a small constant (usually 1) to all counts. It ensures that unseen feature combinations do not lead to a probability of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q66. Can Naïve Bayes handle continuous features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yes, Naïve Bayes can handle continuous features by assuming a Gaussian (normal) distribution for the features and using Gaussian Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q67. What are the assumptions of the Naïve Bayes algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aïve Bayes assumes:\n",
    "\n",
    "    * Conditional Independence: Features are independent given the class label.\n",
    "    * The class-conditional distribution of features follows a specific form (e.g., Gaussian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q68. How does Naïve Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes can handle missing values by ignoring the missing feature when calculating probabilities or by imputing the missing value with statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q69. What are some common applications of Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common applications include:\n",
    "\n",
    "    * Spam filtering.\n",
    "    * Sentiment analysis.\n",
    "    * Document classification.\n",
    "    * Medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q70. Explain the difference between generative and discriminative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generative Models model the joint probability distribution of input features and output labels.\n",
    "* Discriminative Models model the conditional probability of the output given the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q71. How does the decision boundary of a Naïve Bayes classifier look like for binary classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The decision boundary of a Naïve Bayes classifier is typically linear, but it can be non-linear depending on the distribution of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q72. What is the difference between multinomial Naïve Bayes and Gaussian Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multinomial Naïve Bayes is used for discrete count data, such as word frequencies in text classification.\n",
    "* Gaussian Naïve Bayes is used for continuous data, assuming a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q73. How does Naïve Bayes handle numerical instability issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes handles numerical instability by using logarithms of probabilities rather than raw probabilities, which prevents underflow in cases of very small probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q74. What is the Laplacian correction, and when is it used in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Laplacian Correction is another term for Laplace smoothing. It is used to avoid zero probabilities by adding a small constant to all counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q75. Can Naïve Bayes be used for regression tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes is primarily a classification algorithm and is not typically used for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q76. Explain the concept of conditional independence assumption in Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Conditional Independence Assumption in Naïve Bayes states that all features are independent of each other, given the class label. This simplifies the computation of probabilities but may not always hold in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q77. How does Naïve Bayes handle categorical features with a large number of categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes handles categorical features with many categories by estimating the probability for each category separately, but it may become less effective due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q78. What are some drawbacks of the Naïve Bayes algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drawbacks of Naïve Bayes include:\n",
    "    * Assumes feature independence, which is often not true in real-world data.\n",
    "    * Can be less accurate than more complex algorithms.\n",
    "    * Sensitive to the choice of probability distribution for continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q79. Explain the concept of smoothing in Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Smoothing techniques like Laplace smoothing are used to handle zero-frequency problems by ensuring that every possible outcome has a non-zero probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q80. How does Naïve Bayes handle imbalanced datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naïve Bayes can handle imbalanced datasets by incorporating prior probabilities based on the class distribution, but its performance may still degrade if the imbalance is severe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
